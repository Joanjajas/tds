{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5d625-6ba0-44e4-8e7b-b2eaceddf7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "from scipy.io import loadmat\n",
    "from tqdm import tqdm\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71166b74-d272-4c56-b6f0-e9babaf4976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=85,\n",
    "        hidden_dim=[\n",
    "            4,\n",
    "        ],\n",
    "        activation=nn.ReLU(),\n",
    "        out_dim=10,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        assert len(hidden_dim) > 0, \"at least one hidden layer\"\n",
    "        layers = []\n",
    "\n",
    "        # first layer\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim[0]))\n",
    "        layers.append(nn.BatchNorm1d(hidden_dim[0]))\n",
    "\n",
    "        # dropout\n",
    "        if dropout > 0.0:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        # activation\n",
    "        layers.append(activation)\n",
    "\n",
    "        # hidden layers\n",
    "        if len(hidden_dim) > 1:\n",
    "            for k in range(1, len(hidden_dim)):\n",
    "                layers.append(nn.Linear(hidden_dim[k - 1], hidden_dim[k]))\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim[k]))\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "                layers.append(activation)\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim[-1], out_dim))\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14707f81-ce61-4243-99a6-6529016a597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(network, loss_fn, dataloader, optimizer, device=\"cpu\"):\n",
    "    # set the network to train mode\n",
    "    network.to(device)\n",
    "    network.train()\n",
    "\n",
    "    # initialize variables to keep track of loss and number of batches\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    epoch_correct = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    # iterate over the data loader\n",
    "    for batch_inputs, batch_targets in dataloader:\n",
    "        # move data to the appropriate device (e.g., GPU)\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        batch_outputs = network(batch_inputs)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = loss_fn(batch_outputs, batch_targets)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # accumulate the loss\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        # calculate the number of correct predictions in the batch\n",
    "        _, predicted = torch.max(batch_outputs, 1)\n",
    "        epoch_correct += (predicted == batch_targets).sum().item()\n",
    "        num_samples += batch_targets.size(0)\n",
    "\n",
    "    # calculate the average loss for the epoch\n",
    "    average_loss = epoch_loss / num_batches\n",
    "\n",
    "    # calculate the accuracy for the epoch\n",
    "    accuracy = epoch_correct / num_samples\n",
    "\n",
    "    return average_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b65361-b8c7-4a6c-8b54-4a7fdaf3fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(network, loss_fn, dataloader, device=\"cpu\"):\n",
    "    # Set the network to evaluation mode\n",
    "    network.to(device)\n",
    "    network.eval()\n",
    "\n",
    "    # Initialize variables to keep track of loss and number of batches\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    epoch_correct = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    # Turn off gradients\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the data loader\n",
    "        for batch_inputs, batch_targets in dataloader:\n",
    "            # Move data to the appropriate device (e.g., GPU)\n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            batch_outputs = network(batch_inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(batch_outputs, batch_targets)\n",
    "\n",
    "            # Accumulate the loss\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # Calculate the number of correct predictions in the batch\n",
    "            _, predicted = torch.max(batch_outputs, 1)\n",
    "            epoch_correct += (predicted == batch_targets).sum().item()\n",
    "            num_samples += batch_targets.size(0)\n",
    "\n",
    "    # Calculate the average loss for the epoch\n",
    "    average_loss = epoch_loss / num_batches\n",
    "\n",
    "    # Calculate the accuracy for the epoch\n",
    "    accuracy = epoch_correct / num_samples\n",
    "\n",
    "    return average_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd369170-3ae8-4e00-8cc7-2b2b7dc89b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    network,\n",
    "    loss_fn,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    optimizer,\n",
    "    num_epochs,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_acc = []\n",
    "    train_acc = []\n",
    "\n",
    "    # Initialize tqdm for progress tracking\n",
    "    progress_bar = tqdm(range(num_epochs), desc=\"Training Progress\")\n",
    "\n",
    "    for epoch in progress_bar:\n",
    "        # Training phase\n",
    "        train_loss, train_accuracy = train_epoch(\n",
    "            network, loss_fn, train_dataloader, optimizer, device=device\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        train_acc.append(train_accuracy)\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_accuracy = validate_epoch(\n",
    "            network, loss_fn, val_dataloader, device=device\n",
    "        )\n",
    "        val_losses.append(val_loss)\n",
    "        val_acc.append(val_accuracy)\n",
    "\n",
    "        # Print the loss for each epoch\n",
    "        # print(\n",
    "        #     f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f},Val Loss: {val_loss:.4f} Val Acc: {accuracy:.4f}\"\n",
    "        # )\n",
    "\n",
    "        # Update tqdm progress bar\n",
    "        progress_bar.set_postfix(\n",
    "            {\n",
    "                \"Train Loss\": train_loss,\n",
    "                \"Val Loss\": val_loss,\n",
    "                \"Train Acc\": train_accuracy,\n",
    "                \"Val Acc\": val_accuracy,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, train_acc, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb5bbba",
   "metadata": {},
   "source": [
    "Primero cargando los datos de entrenamiento y de validaci√≥n, los normalizamos y creamos los datasets correspondientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75440180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "dataset_train = loadmat(\"training_data.mat\")\n",
    "dataset_validation = loadmat(\"validation_data.mat\")\n",
    "\n",
    "# extract the training and validation data\n",
    "x_train = dataset_train[\"features_train\"]\n",
    "y_train = dataset_train[\"labels_train_int\"].flatten()\n",
    "x_val = dataset_validation[\"features_validation\"]\n",
    "y_val = dataset_validation[\"labels_validation_int\"].flatten()\n",
    "\n",
    "x_train_mean = x_train.mean(axis=0, keepdims=True)\n",
    "x_train_std = x_train.std(axis=0, keepdims=True)\n",
    "\n",
    "x_train_norm = (x_train - x_train_mean) / x_train_std\n",
    "x_val_norm = (x_val - x_train_mean) / x_train_std\n",
    "\n",
    "# convert lists to tensors\n",
    "x_train_tensor = torch.tensor(x_train_norm, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val_norm, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# create TensorDataset\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71d7e8f-6447-41b1-af59-a6de31859a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NetParams:\n",
    "    learning_rate: float\n",
    "    batch_size: int\n",
    "    num_epochs: int\n",
    "    hidden_dim: List[int]\n",
    "    activation: nn.Module\n",
    "    dropout: float\n",
    "\n",
    "\n",
    "def create_net(net_params: NetParams):\n",
    "    # define network\n",
    "    net = Net(\n",
    "        hidden_dim=net_params.hidden_dim,\n",
    "        activation=net_params.activation,\n",
    "        dropout=net_params.dropout,\n",
    "    )\n",
    "\n",
    "    # create the optimizer and loss function\n",
    "    optimizer = optim.SGD(net.parameters(), lr=net_params.learning_rate, momentum=0.85)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # create the data loaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=net_params.batch_size, shuffle=True\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, batch_size=net_params.batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    # train the net\n",
    "    train_losses, val_losses, train_acc, val_acc = train(\n",
    "        net,\n",
    "        loss_fn,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        optimizer,\n",
    "        net_params.num_epochs,\n",
    "        device=\"cpu\",\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Best validation accuracy: {max(val_acc) * 100:.2f}% in epoch {val_acc.index(max(val_acc)) + 1}\"\n",
    "    )\n",
    "\n",
    "    print(f\"Last validation accuracy: {val_acc[-1] * 100:.2f}%\")\n",
    "\n",
    "    # plot the results\n",
    "    plt.plot(train_losses, label=\"train loss\")\n",
    "    plt.plot(val_losses, label=\"val_loss\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(train_acc, label=\"train acc\")\n",
    "    ax.plot(val_acc, label=\"val acc\")\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "    ax.set_ylabel(\"accuracy\")\n",
    "    plt.show()\n",
    "\n",
    "    return val_acc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f25dce8",
   "metadata": {},
   "source": [
    "Creamos nuestra primera red con las siguientes caracter√≠sticas:\n",
    "- Optimizador: SGD con momentum ('sgdm')\n",
    "- Tasa de aprendizaje inicial: 0.01\n",
    "- N√∫mero de √©pocas: 10\n",
    "- Tama√±o del batch: 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815511dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_accuracies = []\n",
    "\n",
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=512,\n",
    "    num_epochs=10,\n",
    "    hidden_dim=[4],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "val_accuracy = create_net(net_params)\n",
    "net_accuracies.append((val_accuracy, net_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f20bd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=10,\n",
    "    batch_size=512,\n",
    "    num_epochs=10,\n",
    "    hidden_dim=[4],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "val_accuracy = create_net(net_params)\n",
    "net_accuracies.append((val_accuracy, net_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af89d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=512,\n",
    "    num_epochs=100,\n",
    "    hidden_dim=[4],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "val_accuracy = create_net(net_params)\n",
    "net_accuracies.append((val_accuracy, net_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f9bcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=512,\n",
    "    num_epochs=70,\n",
    "    hidden_dim=[4],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "val_accuracy = create_net(net_params)\n",
    "net_accuracies.append((val_accuracy, net_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=128,\n",
    "    num_epochs=70,\n",
    "    hidden_dim=[4],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "val_accuracy = create_net(net_params)\n",
    "net_accuracies.append((val_accuracy, net_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3509db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=1024,\n",
    "    num_epochs=70,\n",
    "    hidden_dim=[4],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "val_accuracy = create_net(net_params)\n",
    "net_accuracies.append((val_accuracy, net_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b1e189",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=1024,\n",
    "    num_epochs=150,\n",
    "    hidden_dim=[4],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "val_accuracy = create_net(net_params)\n",
    "net_accuracies.append((val_accuracy, net_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfef39bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=512,\n",
    "    num_epochs=100,\n",
    "    hidden_dim=[4, 4],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "val_accuracy = create_net(net_params)\n",
    "net_accuracies.append((val_accuracy, net_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f0fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=512,\n",
    "    num_epochs=100,\n",
    "    hidden_dim=[128, 128],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "val_accuracy = create_net(net_params)\n",
    "net_accuracies.append((val_accuracy, net_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147cd92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=512,\n",
    "    num_epochs=100,\n",
    "    hidden_dim=[128, 128],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.25,\n",
    ")\n",
    "\n",
    "val_accuracy = create_net(net_params)\n",
    "net_accuracies.append((val_accuracy, net_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced89552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the nets by accuracy order and their parameters\n",
    "net_accuracies.sort(reverse=True)\n",
    "for val_accuracy, net_params in net_accuracies:\n",
    "    print(f\"Validation accuracy: {val_accuracy * 100:.2f}%\")\n",
    "    print(net_params)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0938241d",
   "metadata": {},
   "source": [
    "Para probar el modelo, dado que la red se ha entrenado con los vectores de features calculados en matlab, voy a calcular en matlab tambi√©n el vector de features de los audios de prueba, ejecutando un script de matlab para extraer las features desde python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0667d036",
   "metadata": {},
   "outputs": [],
   "source": [
    "MATLAB_PATH = \"/Applications/MATLAB_R2024a.app/bin/matlab\"  # path to MATLAB executable(could be different on your machine)\n",
    "AUDIO_PATH = \"../segmented_digits/\"  # path to new audios not included in the training or the validation set\n",
    "\n",
    "# Define learning rate\n",
    "learning_rate = 0.02\n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "dropout = 0.15\n",
    "hidden_dim = [128, 128, 128]\n",
    "activation = nn.ELU()\n",
    "\n",
    "# # extract features from the new audio files\n",
    "# command = f\"{MATLAB_PATH} -batch \\\"extract_features('{AUDIO_PATH}')\\\"\"\n",
    "# subprocess.run(command, shell=True)\n",
    "\n",
    "# load the new data\n",
    "descriptors = loadmat(\"test_data.mat\")[\"descriptor\"]\n",
    "ground_truths = loadmat(\"test_data.mat\")[\"groundTruth\"]\n",
    "\n",
    "# normalize the data\n",
    "descriptors = (descriptors - x_train_mean) / x_train_std\n",
    "\n",
    "testset = TensorDataset(\n",
    "    torch.tensor(descriptors, dtype=torch.float32),\n",
    "    torch.tensor(ground_truths, dtype=torch.uint8),\n",
    ")\n",
    "testloader = DataLoader(testset, batch_size=1, shuffle=False)\n",
    "\n",
    "net = Net(hidden_dim=hidden_dim, dropout=dropout, activation=activation)\n",
    "net.load_state_dict(torch.load(\"model.pth\"))\n",
    "net.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for num, ground_truth in testloader:\n",
    "        outputs = net(num)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        prediction = predicted.item()\n",
    "        predictions.append(prediction)\n",
    "\n",
    "        predictions_confidence = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        confidence = torch.max(predictions_confidence)\n",
    "\n",
    "        # print(\n",
    "        #     f\"Predicted digit: {prediction} with confidence of {confidence * 100:.2f}%\"\n",
    "        # )\n",
    "        # print(f\"Ground truth: {ground_truth.item()}\")\n",
    "        # print(\"\\n====================================================\\n\")\n",
    "\n",
    "accuracy = np.mean(ground_truths.flatten() == predictions)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(ground_truths, predictions)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix,\n",
    "    display_labels=[\n",
    "        \"cero\",\n",
    "        \"uno\",\n",
    "        \"dos\",\n",
    "        \"tres\",\n",
    "        \"cuatro\",\n",
    "        \"cinco\",\n",
    "        \"seis\",\n",
    "        \"siete\",\n",
    "        \"ocho\",\n",
    "        \"nueve\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "cm_display.plot()\n",
    "cm_display.figure_.set_size_inches(10, 8)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
