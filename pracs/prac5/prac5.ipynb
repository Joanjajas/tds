{
 "cells": [
  {
   "cell_type": "code",
   "id": "76e5d625-6ba0-44e4-8e7b-b2eaceddf7a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T21:24:47.486368Z",
     "start_time": "2024-05-21T21:24:47.359749Z"
    }
   },
   "source": [
    "import torch\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import sounddevice as sd\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from scipy.io.wavfile import write\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "from scipy.io import loadmat\n",
    "from tqdm import tqdm\n",
    "from torch import nn"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msubprocess\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'torch'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "71166b74-d272-4c56-b6f0-e9babaf4976c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T21:24:47.489029Z",
     "start_time": "2024-05-21T21:24:47.488959Z"
    }
   },
   "source": [
    "class Net(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=85,\n",
    "        hidden_dim=[4],\n",
    "        activation=nn.ReLU(),\n",
    "        out_dim=10,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        assert len(hidden_dim) > 0, \"at least one hidden layer\"\n",
    "        layers = []\n",
    "\n",
    "        # first layer\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim[0]))\n",
    "        layers.append(nn.BatchNorm1d(hidden_dim[0]))\n",
    "\n",
    "        # dropout\n",
    "        if dropout > 0.0:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        # activation\n",
    "        layers.append(activation)\n",
    "\n",
    "        # hidden layers\n",
    "        if len(hidden_dim) > 1:\n",
    "            for k in range(1, len(hidden_dim)):\n",
    "                layers.append(nn.Linear(hidden_dim[k - 1], hidden_dim[k]))\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim[k]))\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "                layers.append(activation)\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim[-1], out_dim))\n",
    "        super().__init__(*layers)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "14707f81-ce61-4243-99a6-6529016a597c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T21:24:47.489728Z",
     "start_time": "2024-05-21T21:24:47.489659Z"
    }
   },
   "source": [
    "def train_epoch(network, loss_fn, dataloader, optimizer, device=\"cpu\"):\n",
    "    # set the network to train mode\n",
    "    network.to(device)\n",
    "    network.train()\n",
    "\n",
    "    # initialize variables to keep track of loss and number of batches\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    epoch_correct = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    # iterate over the data loader\n",
    "    for batch_inputs, batch_targets in dataloader:\n",
    "        # move data to the appropriate device (e.g., GPU)\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        batch_outputs = network(batch_inputs)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = loss_fn(batch_outputs, batch_targets)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # accumulate the loss\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        # calculate the number of correct predictions in the batch\n",
    "        _, predicted = torch.max(batch_outputs, 1)\n",
    "        epoch_correct += (predicted == batch_targets).sum().item()\n",
    "        num_samples += batch_targets.size(0)\n",
    "\n",
    "    # calculate the average loss for the epoch\n",
    "    average_loss = epoch_loss / num_batches\n",
    "\n",
    "    # calculate the accuracy for the epoch\n",
    "    accuracy = epoch_correct / num_samples\n",
    "\n",
    "    return average_loss, accuracy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "78b65361-b8c7-4a6c-8b54-4a7fdaf3fd85",
   "metadata": {},
   "source": [
    "def validate_epoch(network, loss_fn, dataloader, device=\"cpu\"):\n",
    "    # Set the network to evaluation mode\n",
    "    network.to(device)\n",
    "    network.eval()\n",
    "\n",
    "    # Initialize variables to keep track of loss and number of batches\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    epoch_correct = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    # Turn off gradients\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the data loader\n",
    "        for batch_inputs, batch_targets in dataloader:\n",
    "            # Move data to the appropriate device (e.g., GPU)\n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            batch_outputs = network(batch_inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(batch_outputs, batch_targets)\n",
    "\n",
    "            # Accumulate the loss\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # Calculate the number of correct predictions in the batch\n",
    "            _, predicted = torch.max(batch_outputs, 1)\n",
    "            epoch_correct += (predicted == batch_targets).sum().item()\n",
    "            num_samples += batch_targets.size(0)\n",
    "\n",
    "    # Calculate the average loss for the epoch\n",
    "    average_loss = epoch_loss / num_batches\n",
    "\n",
    "    # Calculate the accuracy for the epoch\n",
    "    accuracy = epoch_correct / num_samples\n",
    "\n",
    "    return average_loss, accuracy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bd369170-3ae8-4e00-8cc7-2b2b7dc89b20",
   "metadata": {},
   "source": [
    "def train(\n",
    "    network,\n",
    "    loss_fn,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    optimizer,\n",
    "    num_epochs,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_acc = []\n",
    "    train_acc = []\n",
    "\n",
    "    # Initialize tqdm for progress tracking\n",
    "    progress_bar = tqdm(range(num_epochs), desc=\"Training Progress\")\n",
    "\n",
    "    for _ in progress_bar:\n",
    "        # Training phase\n",
    "        train_loss, train_accuracy = train_epoch(\n",
    "            network, loss_fn, train_dataloader, optimizer, device=device\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        train_acc.append(train_accuracy)\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_accuracy = validate_epoch(\n",
    "            network, loss_fn, val_dataloader, device=device\n",
    "        )\n",
    "        val_losses.append(val_loss)\n",
    "        val_acc.append(val_accuracy)\n",
    "\n",
    "        # Update tqdm progress bar\n",
    "        progress_bar.set_postfix(\n",
    "            {\n",
    "                \"Train Loss\": train_loss,\n",
    "                \"Val Loss\": val_loss,\n",
    "                \"Train Acc\": train_accuracy,\n",
    "                \"Val Acc\": val_accuracy,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, train_acc, val_acc"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "abb5bbba",
   "metadata": {},
   "source": [
    "Primero cargamos los datos de entrenamiento y de validación, los normalizamos y creamos los datasets correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "id": "75440180",
   "metadata": {},
   "source": [
    "# load the dataset\n",
    "dataset_train = loadmat(\"training_data.mat\")\n",
    "dataset_validation = loadmat(\"validation_data.mat\")\n",
    "\n",
    "# extract the training and validation data\n",
    "x_train = dataset_train[\"features_train\"]\n",
    "y_train = dataset_train[\"labels_train_int\"].flatten()\n",
    "x_val = dataset_validation[\"features_validation\"]\n",
    "y_val = dataset_validation[\"labels_validation_int\"].flatten()\n",
    "\n",
    "x_train_mean = x_train.mean(axis=0, keepdims=True)\n",
    "x_train_std = x_train.std(axis=0, keepdims=True)\n",
    "\n",
    "x_train_norm = (x_train - x_train_mean) / x_train_std\n",
    "x_val_norm = (x_val - x_train_mean) / x_train_std\n",
    "\n",
    "# convert lists to tensors\n",
    "x_train_tensor = torch.tensor(x_train_norm, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val_norm, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# create TensorDataset\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e6bdcb35",
   "metadata": {},
   "source": [
    "Vamos a definir una función para crear nuestra red dados unos parametros, ya que vamos a ir creando varias redes para probar distintas configuraciones de los parametros de la red."
   ]
  },
  {
   "cell_type": "code",
   "id": "e71d7e8f-6447-41b1-af59-a6de31859a04",
   "metadata": {},
   "source": [
    "@dataclass\n",
    "class NetParams:\n",
    "    learning_rate: float\n",
    "    batch_size: int\n",
    "    num_epochs: int\n",
    "    hidden_dim: List[int]\n",
    "    activation: nn.Module\n",
    "    dropout: float\n",
    "\n",
    "\n",
    "def create_net(net_params: NetParams):\n",
    "    # define network\n",
    "    net = Net(\n",
    "        hidden_dim=net_params.hidden_dim,\n",
    "        activation=net_params.activation,\n",
    "        dropout=net_params.dropout,\n",
    "    )\n",
    "\n",
    "    # create the optimizer and loss function\n",
    "    optimizer = optim.SGD(net.parameters(), lr=net_params.learning_rate, momentum=0.85)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # create the data loaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=net_params.batch_size, shuffle=True\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, batch_size=net_params.batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    # train the net\n",
    "    train_losses, val_losses, train_acc, val_acc = train(\n",
    "        net,\n",
    "        loss_fn,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        optimizer,\n",
    "        net_params.num_epochs,\n",
    "        device=\"cpu\",\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Best validation accuracy: {max(val_acc) * 100:.2f}% in epoch {val_acc.index(max(val_acc)) + 1}\"\n",
    "    )\n",
    "\n",
    "    print(f\"Last validation accuracy: {val_acc[-1] * 100:.2f}%\")\n",
    "\n",
    "    # plot the results\n",
    "    plt.plot(train_losses, label=\"train loss\")\n",
    "    plt.plot(val_losses, label=\"val_loss\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(train_acc, label=\"train acc\")\n",
    "    ax.plot(val_acc, label=\"val acc\")\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "    ax.set_ylabel(\"accuracy\")\n",
    "    plt.show()\n",
    "\n",
    "    return net, val_acc[-1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2f25dce8",
   "metadata": {},
   "source": [
    "Creamos nuestra primera red con las siguientes características:\n",
    "- Optimizador: SGD con momentum ('sgdm')\n",
    "- Tasa de aprendizaje inicial: 0.01\n",
    "- Número de épocas: 10\n",
    "- Tamaño del batch: 512"
   ]
  },
  {
   "cell_type": "code",
   "id": "815511dd",
   "metadata": {},
   "source": [
    "models = []\n",
    "\n",
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=512,\n",
    "    num_epochs=10,\n",
    "    hidden_dim=[4],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "net, val_accuracy = create_net(net_params)\n",
    "models.append((net, val_accuracy, net_params))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8e558277",
   "metadata": {},
   "source": [
    "Ahora vamos a probar subiendo el learning rate a 10. Vemos como la accuracy pasa a ser del 10%, el peor resultado que se puede conseguir como si la red estuviera clasificando los números aleatoriamente."
   ]
  },
  {
   "cell_type": "code",
   "id": "3f20bd2d",
   "metadata": {},
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=10,\n",
    "    batch_size=512,\n",
    "    num_epochs=10,\n",
    "    hidden_dim=[4],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "net, val_accuracy = create_net(net_params)\n",
    "models.append((net, val_accuracy, net_params))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd447231",
   "metadata": {},
   "source": [
    "Volviendo a poner el learning rate a 0.01 y subiendo el número de epocas a 100 conseguimos elevar el accuracy del modelo."
   ]
  },
  {
   "cell_type": "code",
   "id": "2af89d31",
   "metadata": {},
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=512,\n",
    "    num_epochs=100,\n",
    "    hidden_dim=[4],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "net, val_accuracy = create_net(net_params)\n",
    "models.append((net, val_accuracy, net_params))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "722578e5",
   "metadata": {},
   "source": [
    "Ahora al reducir las épocas a 70 conseguimos una accuracy similar. En cuanto al tiempo de entrenamiento a disminuido respecto al del modelo anterior."
   ]
  },
  {
   "cell_type": "code",
   "id": "f7f9bcaa",
   "metadata": {},
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=512,\n",
    "    num_epochs=70,\n",
    "    hidden_dim=[4],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "net, val_accuracy = create_net(net_params)\n",
    "models.append((net, val_accuracy, net_params))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "abe9fc9a",
   "metadata": {},
   "source": [
    "Al reducir el tamaño del batch el accuracy aumenta al igual que el tiempo de entrenamiento del modelo."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=128,\n",
    "    num_epochs=70,\n",
    "    hidden_dim=[4],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "net, val_accuracy = create_net(net_params)\n",
    "models.append((net, val_accuracy, net_params))"
   ],
   "id": "39bc268f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3b5b1ac1",
   "metadata": {},
   "source": [
    "Cuando aumentamos el tamaño del batch el tiempo de entrenamiento se ver reducido y la accuracy baja un poco ya que la red no tiene el suficiente número de épocas para entrenar."
   ]
  },
  {
   "cell_type": "code",
   "id": "f3509db6",
   "metadata": {},
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=1024,\n",
    "    num_epochs=70,\n",
    "    hidden_dim=[4],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "net, val_accuracy = create_net(net_params)\n",
    "models.append((net, val_accuracy, net_params))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "86ae99e5",
   "metadata": {},
   "source": [
    "Cuando aumentamos el número de épocas aumenta también el tiempo de entrenamiento pero conseguimos una accuracy mayor."
   ]
  },
  {
   "cell_type": "code",
   "id": "81b1e189",
   "metadata": {},
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=1024,\n",
    "    num_epochs=150,\n",
    "    hidden_dim=[4],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "net, val_accuracy = create_net(net_params)\n",
    "models.append((net, val_accuracy, net_params))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "34cf0b15",
   "metadata": {},
   "source": [
    "Conforme aumentamos el número de capas y el número de neuronas en cada capa el accuracy del modelo va aumentando."
   ]
  },
  {
   "cell_type": "code",
   "id": "dfef39bd",
   "metadata": {},
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=512,\n",
    "    num_epochs=100,\n",
    "    hidden_dim=[4, 4],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "net, val_accuracy = create_net(net_params)\n",
    "models.append((net, val_accuracy, net_params))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "74f0fbcc",
   "metadata": {},
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=512,\n",
    "    num_epochs=100,\n",
    "    hidden_dim=[128, 128],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "net, val_accuracy = create_net(net_params)\n",
    "models.append((net, val_accuracy, net_params))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "85e692ff",
   "metadata": {},
   "source": [
    "En la última red podemos ver que la accuracy de entrenamiento acaba siendo del 100% lo que puede ser perjudicial. Para corregir esto añadimos una capa de dropout lo que produce un aumento en el accuracy."
   ]
  },
  {
   "cell_type": "code",
   "id": "147cd92b",
   "metadata": {},
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=512,\n",
    "    num_epochs=150,\n",
    "    hidden_dim=[128, 128],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.25,\n",
    ")\n",
    "\n",
    "net, val_accuracy = create_net(net_params)\n",
    "models.append((net, val_accuracy, net_params))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e165b9b6",
   "metadata": {},
   "source": [
    "Ahora vamos a comparar los distintos modelos y cual a sido du accuracy en función de sus parámetros."
   ]
  },
  {
   "cell_type": "code",
   "id": "ced89552",
   "metadata": {},
   "source": [
    "# print the nets by accuracy order and their parameters\n",
    "models.sort(key=lambda x: x[1], reverse=True)\n",
    "for _, val_accuracy, net_params in models:\n",
    "    print(f\"Validation accuracy: {val_accuracy * 100:.2f}%\")\n",
    "    print(net_params)\n",
    "    print()\n",
    "\n",
    "# keep the best model and save it\n",
    "net = models[0][0]\n",
    "torch.save(net.state_dict(), \"model.pth\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0938241d",
   "metadata": {},
   "source": [
    "Para probar el modelo, dado que la red se ha entrenado con los vectores de features calculados en matlab, voy a calcular en matlab también el vector de features de los audios de prueba, ejecutando un script de matlab desde python. Para probar la red en tiempo real hay que ejecutar esta celda y decir un dígito en los siguientes 3 segundos. Dado que por detras se tiene que iniciar y ejecutar matlab para calcular el vector de features el proceso es un poco lento."
   ]
  },
  {
   "cell_type": "code",
   "id": "0667d036",
   "metadata": {},
   "source": [
    "MATLAB_PATH = \"/Applications/MATLAB_R2024a.app/bin/matlab\"  # path to MATLAB executable(could be different on your machine)\n",
    "AUDIO_PATH = \"spoken_digit.wav\"\n",
    "\n",
    "freq = 16000  # sample rate\n",
    "duration = 3  # seconds\n",
    "\n",
    "# record the audio\n",
    "record = sd.rec(int(duration * freq), samplerate=freq, channels=1)\n",
    "sd.wait()\n",
    "\n",
    "# save the audio\n",
    "write(AUDIO_PATH, 16000, record)\n",
    "\n",
    "# extract features from the audio file and save it to a .mat file\n",
    "command = f\"{MATLAB_PATH} -batch \\\"extract_features('{AUDIO_PATH}')\\\"\"\n",
    "subprocess.run(command, shell=True)\n",
    "\n",
    "# load the audio descriptor\n",
    "descriptor = loadmat(\"test_data.mat\")[\"descriptor\"]\n",
    "\n",
    "# normalize the descriptor\n",
    "descriptor = (descriptor - x_train_mean) / x_train_std\n",
    "\n",
    "# create a test set\n",
    "testset = TensorDataset(\n",
    "    torch.tensor(descriptor, dtype=torch.float32),\n",
    ")\n",
    "testloader = DataLoader(testset, batch_size=1, shuffle=False)\n",
    "\n",
    "# load the trained model\n",
    "net.load_state_dict(torch.load(\"model.pth\"))\n",
    "net.eval()\n",
    "\n",
    "# make the prediction\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for num in testloader:\n",
    "        outputs = net(num[0])\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        prediction = predicted.item()\n",
    "        predictions.append(prediction)\n",
    "\n",
    "        predictions_confidence = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        confidence = torch.max(predictions_confidence)\n",
    "\n",
    "        print(\n",
    "            f\"Predicted digit: {prediction} with confidence of {confidence * 100:.2f}%\"\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
