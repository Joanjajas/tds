{
 "cells": [
  {
   "cell_type": "code",
   "id": "76e5d625-6ba0-44e4-8e7b-b2eaceddf7a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T21:24:47.486368Z",
     "start_time": "2024-05-21T21:24:47.359749Z"
    }
   },
   "source": [
    "import torch\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import sounddevice as sd\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from scipy.io.wavfile import write\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "from scipy.io import loadmat\n",
    "from tqdm import tqdm\n",
    "from torch import nn"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msubprocess\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'torch'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "71166b74-d272-4c56-b6f0-e9babaf4976c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T21:24:47.489029Z",
     "start_time": "2024-05-21T21:24:47.488959Z"
    }
   },
   "source": [
    "class Net(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=85,\n",
    "        hidden_dim=[4],\n",
    "        activation=nn.ReLU(),\n",
    "        out_dim=10,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        assert len(hidden_dim) > 0, \"at least one hidden layer\"\n",
    "        layers = []\n",
    "\n",
    "        # first layer\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim[0]))\n",
    "        layers.append(nn.BatchNorm1d(hidden_dim[0]))\n",
    "\n",
    "        # dropout\n",
    "        if dropout > 0.0:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        # activation\n",
    "        layers.append(activation)\n",
    "\n",
    "        # hidden layers\n",
    "        if len(hidden_dim) > 1:\n",
    "            for k in range(1, len(hidden_dim)):\n",
    "                layers.append(nn.Linear(hidden_dim[k - 1], hidden_dim[k]))\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim[k]))\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "                layers.append(activation)\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim[-1], out_dim))\n",
    "        super().__init__(*layers)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "14707f81-ce61-4243-99a6-6529016a597c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T21:24:47.489728Z",
     "start_time": "2024-05-21T21:24:47.489659Z"
    }
   },
   "source": [
    "def train_epoch(network, loss_fn, dataloader, optimizer, device=\"cpu\"):\n",
    "    # set the network to train mode\n",
    "    network.to(device)\n",
    "    network.train()\n",
    "\n",
    "    # initialize variables to keep track of loss and number of batches\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    epoch_correct = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    # iterate over the data loader\n",
    "    for batch_inputs, batch_targets in dataloader:\n",
    "        # move data to the appropriate device (e.g., GPU)\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        batch_outputs = network(batch_inputs)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = loss_fn(batch_outputs, batch_targets)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # accumulate the loss\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        # calculate the number of correct predictions in the batch\n",
    "        _, predicted = torch.max(batch_outputs, 1)\n",
    "        epoch_correct += (predicted == batch_targets).sum().item()\n",
    "        num_samples += batch_targets.size(0)\n",
    "\n",
    "    # calculate the average loss for the epoch\n",
    "    average_loss = epoch_loss / num_batches\n",
    "\n",
    "    # calculate the accuracy for the epoch\n",
    "    accuracy = epoch_correct / num_samples\n",
    "\n",
    "    return average_loss, accuracy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "78b65361-b8c7-4a6c-8b54-4a7fdaf3fd85",
   "metadata": {},
   "source": [
    "def validate_epoch(network, loss_fn, dataloader, device=\"cpu\"):\n",
    "    # Set the network to evaluation mode\n",
    "    network.to(device)\n",
    "    network.eval()\n",
    "\n",
    "    # Initialize variables to keep track of loss and number of batches\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    epoch_correct = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    # Turn off gradients\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the data loader\n",
    "        for batch_inputs, batch_targets in dataloader:\n",
    "            # Move data to the appropriate device (e.g., GPU)\n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            batch_outputs = network(batch_inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(batch_outputs, batch_targets)\n",
    "\n",
    "            # Accumulate the loss\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # Calculate the number of correct predictions in the batch\n",
    "            _, predicted = torch.max(batch_outputs, 1)\n",
    "            epoch_correct += (predicted == batch_targets).sum().item()\n",
    "            num_samples += batch_targets.size(0)\n",
    "\n",
    "    # Calculate the average loss for the epoch\n",
    "    average_loss = epoch_loss / num_batches\n",
    "\n",
    "    # Calculate the accuracy for the epoch\n",
    "    accuracy = epoch_correct / num_samples\n",
    "\n",
    "    return average_loss, accuracy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bd369170-3ae8-4e00-8cc7-2b2b7dc89b20",
   "metadata": {},
   "source": [
    "def train(\n",
    "    network,\n",
    "    loss_fn,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    optimizer,\n",
    "    num_epochs,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_acc = []\n",
    "    train_acc = []\n",
    "\n",
    "    # Initialize tqdm for progress tracking\n",
    "    progress_bar = tqdm(range(num_epochs), desc=\"Training Progress\")\n",
    "\n",
    "    for _ in progress_bar:\n",
    "        # Training phase\n",
    "        train_loss, train_accuracy = train_epoch(\n",
    "            network, loss_fn, train_dataloader, optimizer, device=device\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        train_acc.append(train_accuracy)\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_accuracy = validate_epoch(\n",
    "            network, loss_fn, val_dataloader, device=device\n",
    "        )\n",
    "        val_losses.append(val_loss)\n",
    "        val_acc.append(val_accuracy)\n",
    "\n",
    "        # Update tqdm progress bar\n",
    "        progress_bar.set_postfix(\n",
    "            {\n",
    "                \"Train Loss\": train_loss,\n",
    "                \"Val Loss\": val_loss,\n",
    "                \"Train Acc\": train_accuracy,\n",
    "                \"Val Acc\": val_accuracy,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, train_acc, val_acc"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "abb5bbba",
   "metadata": {},
   "source": [
    "Primero cargamos los datos de entrenamiento y de validaci√≥n, los normalizamos y creamos los datasets correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "id": "75440180",
   "metadata": {},
   "source": [
    "# load the dataset\n",
    "dataset_train = loadmat(\"training_data.mat\")\n",
    "dataset_validation = loadmat(\"validation_data.mat\")\n",
    "\n",
    "# extract the training and validation data\n",
    "x_train = dataset_train[\"features_train\"]\n",
    "y_train = dataset_train[\"labels_train_int\"].flatten()\n",
    "x_val = dataset_validation[\"features_validation\"]\n",
    "y_val = dataset_validation[\"labels_validation_int\"].flatten()\n",
    "\n",
    "x_train_mean = x_train.mean(axis=0, keepdims=True)\n",
    "x_train_std = x_train.std(axis=0, keepdims=True)\n",
    "\n",
    "x_train_norm = (x_train - x_train_mean) / x_train_std\n",
    "x_val_norm = (x_val - x_train_mean) / x_train_std\n",
    "\n",
    "# convert lists to tensors\n",
    "x_train_tensor = torch.tensor(x_train_norm, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val_norm, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# create TensorDataset\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e6bdcb35",
   "metadata": {},
   "source": [
    "Vamos a definir una funci√≥n para crear nuestra red dados unos parametros, ya que vamos a ir creando varias redes para probar distintas configuraciones de los parametros de la red."
   ]
  },
  {
   "cell_type": "code",
   "id": "e71d7e8f-6447-41b1-af59-a6de31859a04",
   "metadata": {},
   "source": [
    "@dataclass\n",
    "class NetParams:\n",
    "    learning_rate: float\n",
    "    batch_size: int\n",
    "    num_epochs: int\n",
    "    hidden_dim: List[int]\n",
    "    activation: nn.Module\n",
    "    dropout: float\n",
    "\n",
    "\n",
    "def create_net(net_params: NetParams):\n",
    "    # define network\n",
    "    net = Net(\n",
    "        hidden_dim=net_params.hidden_dim,\n",
    "        activation=net_params.activation,\n",
    "        dropout=net_params.dropout,\n",
    "    )\n",
    "\n",
    "    # create the optimizer and loss function\n",
    "    optimizer = optim.SGD(net.parameters(), lr=net_params.learning_rate, momentum=0.85)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # create the data loaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=net_params.batch_size, shuffle=True\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, batch_size=net_params.batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    # train the net\n",
    "    train_losses, val_losses, train_acc, val_acc = train(\n",
    "        net,\n",
    "        loss_fn,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        optimizer,\n",
    "        net_params.num_epochs,\n",
    "        device=\"cpu\",\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Best validation accuracy: {max(val_acc) * 100:.2f}% in epoch {val_acc.index(max(val_acc)) + 1}\"\n",
    "    )\n",
    "\n",
    "    print(f\"Last validation accuracy: {val_acc[-1] * 100:.2f}%\")\n",
    "\n",
    "    # plot the results\n",
    "    plt.plot(train_losses, label=\"train loss\")\n",
    "    plt.plot(val_losses, label=\"val_loss\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(train_acc, label=\"train acc\")\n",
    "    ax.plot(val_acc, label=\"val acc\")\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "    ax.set_ylabel(\"accuracy\")\n",
    "    plt.show()\n",
    "\n",
    "    return net, val_acc[-1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2f25dce8",
   "metadata": {},
   "source": [
    "Creamos nuestra primera red con las siguientes caracter√≠sticas:\n",
    "- Optimizador: SGD con momentum ('sgdm')\n",
    "- Tasa de aprendizaje inicial: 0.01\n",
    "- N√∫mero de √©pocas: 10\n",
    "- Tama√±o del batch: 512"
   ]
  },
  {
   "cell_type": "code",
   "id": "815511dd",
   "metadata": {},
   "source": [
    "models = []\n",
    "\n",
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=512,\n",
    "    num_epochs=10,\n",
    "    hidden_dim=[4],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "net, val_accuracy = create_net(net_params)\n",
    "models.append((net, val_accuracy, net_params))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8e558277",
   "metadata": {},
   "source": [
    "Ahora vamos a probar subiendo el learning rate a 10. Vemos como la accuracy pasa a ser del 10%, el peor resultado que se puede conseguir como si la red estuviera clasificando los n√∫meros aleatoriamente."
   ]
  },
  {
   "cell_type": "code",
   "id": "3f20bd2d",
   "metadata": {},
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=10,\n",
    "    batch_size=512,\n",
    "    num_epochs=10,\n",
    "    hidden_dim=[4],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "net, val_accuracy = create_net(net_params)\n",
    "models.append((net, val_accuracy, net_params))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd447231",
   "metadata": {},
   "source": [
    "Volviendo a poner el learning rate a 0.01 y subiendo el n√∫mero de epocas a 100 conseguimos elevar el accuracy del modelo."
   ]
  },
  {
   "cell_type": "code",
   "id": "2af89d31",
   "metadata": {},
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=512,\n",
    "    num_epochs=100,\n",
    "    hidden_dim=[4],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "net, val_accuracy = create_net(net_params)\n",
    "models.append((net, val_accuracy, net_params))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "722578e5",
   "metadata": {},
   "source": [
    "Ahora al reducir las √©pocas a 70 conseguimos una accuracy similar. En cuanto al tiempo de entrenamiento a disminuido respecto al del modelo anterior."
   ]
  },
  {
   "cell_type": "code",
   "id": "f7f9bcaa",
   "metadata": {},
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=512,\n",
    "    num_epochs=70,\n",
    "    hidden_dim=[4],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "net, val_accuracy = create_net(net_params)\n",
    "models.append((net, val_accuracy, net_params))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "abe9fc9a",
   "metadata": {},
   "source": [
    "Al reducir el tama√±o del batch el accuracy aumenta al igual que el tiempo de entrenamiento del modelo."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=128,\n",
    "    num_epochs=70,\n",
    "    hidden_dim=[4],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "net, val_accuracy = create_net(net_params)\n",
    "models.append((net, val_accuracy, net_params))"
   ],
   "id": "39bc268f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3b5b1ac1",
   "metadata": {},
   "source": [
    "Cuando aumentamos el tama√±o del batch el tiempo de entrenamiento se ver reducido y la accuracy baja un poco ya que la red no tiene el suficiente n√∫mero de √©pocas para entrenar."
   ]
  },
  {
   "cell_type": "code",
   "id": "f3509db6",
   "metadata": {},
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=1024,\n",
    "    num_epochs=70,\n",
    "    hidden_dim=[4],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "net, val_accuracy = create_net(net_params)\n",
    "models.append((net, val_accuracy, net_params))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "86ae99e5",
   "metadata": {},
   "source": [
    "Cuando aumentamos el n√∫mero de √©pocas aumenta tambi√©n el tiempo de entrenamiento pero conseguimos una accuracy mayor."
   ]
  },
  {
   "cell_type": "code",
   "id": "81b1e189",
   "metadata": {},
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=1024,\n",
    "    num_epochs=150,\n",
    "    hidden_dim=[4],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "net, val_accuracy = create_net(net_params)\n",
    "models.append((net, val_accuracy, net_params))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "34cf0b15",
   "metadata": {},
   "source": [
    "Conforme aumentamos el n√∫mero de capas y el n√∫mero de neuronas en cada capa el accuracy del modelo va aumentando."
   ]
  },
  {
   "cell_type": "code",
   "id": "dfef39bd",
   "metadata": {},
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=512,\n",
    "    num_epochs=100,\n",
    "    hidden_dim=[4, 4],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "net, val_accuracy = create_net(net_params)\n",
    "models.append((net, val_accuracy, net_params))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "74f0fbcc",
   "metadata": {},
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=512,\n",
    "    num_epochs=100,\n",
    "    hidden_dim=[128, 128],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "net, val_accuracy = create_net(net_params)\n",
    "models.append((net, val_accuracy, net_params))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "85e692ff",
   "metadata": {},
   "source": [
    "En la √∫ltima red podemos ver que la accuracy de entrenamiento acaba siendo del 100% lo que puede ser perjudicial. Para corregir esto a√±adimos una capa de dropout lo que produce un aumento en el accuracy."
   ]
  },
  {
   "cell_type": "code",
   "id": "147cd92b",
   "metadata": {},
   "source": [
    "net_params = NetParams(\n",
    "    learning_rate=0.01,\n",
    "    batch_size=512,\n",
    "    num_epochs=150,\n",
    "    hidden_dim=[128, 128],\n",
    "    activation=nn.ReLU(),\n",
    "    dropout=0.25,\n",
    ")\n",
    "\n",
    "net, val_accuracy = create_net(net_params)\n",
    "models.append((net, val_accuracy, net_params))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e165b9b6",
   "metadata": {},
   "source": [
    "Ahora vamos a comparar los distintos modelos y cual a sido du accuracy en funci√≥n de sus par√°metros."
   ]
  },
  {
   "cell_type": "code",
   "id": "ced89552",
   "metadata": {},
   "source": [
    "# print the nets by accuracy order and their parameters\n",
    "models.sort(key=lambda x: x[1], reverse=True)\n",
    "for _, val_accuracy, net_params in models:\n",
    "    print(f\"Validation accuracy: {val_accuracy * 100:.2f}%\")\n",
    "    print(net_params)\n",
    "    print()\n",
    "\n",
    "# keep the best model and save it\n",
    "net = models[0][0]\n",
    "torch.save(net.state_dict(), \"model.pth\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0938241d",
   "metadata": {},
   "source": [
    "Para probar el modelo, dado que la red se ha entrenado con los vectores de features calculados en matlab, voy a calcular en matlab tambi√©n el vector de features de los audios de prueba, ejecutando un script de matlab desde python. Para probar la red en tiempo real hay que ejecutar esta celda y decir un d√≠gito en los siguientes 3 segundos. Dado que por detras se tiene que iniciar y ejecutar matlab para calcular el vector de features el proceso es un poco lento."
   ]
  },
  {
   "cell_type": "code",
   "id": "0667d036",
   "metadata": {},
   "source": [
    "MATLAB_PATH = \"/Applications/MATLAB_R2024a.app/bin/matlab\"  # path to MATLAB executable(could be different on your machine)\n",
    "AUDIO_PATH = \"spoken_digit.wav\"\n",
    "\n",
    "freq = 16000  # sample rate\n",
    "duration = 3  # seconds\n",
    "\n",
    "# record the audio\n",
    "record = sd.rec(int(duration * freq), samplerate=freq, channels=1)\n",
    "sd.wait()\n",
    "\n",
    "# save the audio\n",
    "write(AUDIO_PATH, 16000, record)\n",
    "\n",
    "# extract features from the audio file and save it to a .mat file\n",
    "command = f\"{MATLAB_PATH} -batch \\\"extract_features('{AUDIO_PATH}')\\\"\"\n",
    "subprocess.run(command, shell=True)\n",
    "\n",
    "# load the audio descriptor\n",
    "descriptor = loadmat(\"test_data.mat\")[\"descriptor\"]\n",
    "\n",
    "# normalize the descriptor\n",
    "descriptor = (descriptor - x_train_mean) / x_train_std\n",
    "\n",
    "# create a test set\n",
    "testset = TensorDataset(\n",
    "    torch.tensor(descriptor, dtype=torch.float32),\n",
    ")\n",
    "testloader = DataLoader(testset, batch_size=1, shuffle=False)\n",
    "\n",
    "# load the trained model\n",
    "net.load_state_dict(torch.load(\"model.pth\"))\n",
    "net.eval()\n",
    "\n",
    "# make the prediction\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for num in testloader:\n",
    "        outputs = net(num[0])\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        prediction = predicted.item()\n",
    "        predictions.append(prediction)\n",
    "\n",
    "        predictions_confidence = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        confidence = torch.max(predictions_confidence)\n",
    "\n",
    "        print(\n",
    "            f\"Predicted digit: {prediction} with confidence of {confidence * 100:.2f}%\"\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
